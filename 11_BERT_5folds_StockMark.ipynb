{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18374,
     "status": "ok",
     "timestamp": 1671933306180,
     "user": {
      "displayName": "kanda taisei",
      "userId": "12664805990869041403"
     },
     "user_tz": -540
    },
    "id": "k2bvGATwoOAf",
    "outputId": "410a7879-e150-4d73-a873-2dd9b71c151f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# This code was written by Taisei KANDA.\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3PRyGSHK_3Y"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU9nZn8JKwxB"
   },
   "outputs": [],
   "source": [
    "# Install morphological analysis library MeCab and dictionary (mecab-ipadic-NEologd)\n",
    "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
    "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null\n",
    "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
    "!pip install mecab-python3==0.996.5 > /dev/null  # mecab-python3の最新版はだめ\n",
    "\n",
    "#  Symbolic links to avoid errors\n",
    "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1671933402401,
     "user": {
      "displayName": "kanda taisei",
      "userId": "12664805990869041403"
     },
     "user_tz": -540
    },
    "id": "tKkr8MeKK2EX",
    "outputId": "dd91b989-bd1a-4abd-e4ac-ccad6ade7007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n"
     ]
    }
   ],
   "source": [
    "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1m7Y1C7PKJb"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install ipadic\n",
    "!pip install unidic-lite\n",
    "!pip install fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9jZWL_f9bg5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertJapaneseTokenizer\n",
    "import MeCab\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW1YOjUClS0A"
   },
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class CreateDataset(Dataset):\n",
    "  def __init__(self, X, y, tokenizer, max_len):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):               #  len(Dataset) to specify the value to return\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):   # Specify the value to return in Dataset[index]\n",
    "    text = self.X[index]\n",
    "\n",
    "    #2022/12/09 Passing list to set encode_plus()'s is_pretokenized to True. . 2022/12/09 Pass list to set encode_plus()'s is_pretokenized to True. \n",
    "    # 2022/12/13 Change to BertJapaneseTokenzier and [UNK] can be removed.\n",
    "    sen = tagger.parse(text)\n",
    "    # print(sen)\n",
    "\n",
    "    inputs = self.tokenizer.encode_plus(\n",
    "      sen,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      # pad_to_max_length=True\n",
    "      truncation = True,  # for transfomers latest ver\n",
    "      padding = \"max_length\" # for transfomers latest ver\n",
    "    )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "\n",
    "    return {\n",
    "      'ids': torch.LongTensor(ids),\n",
    "      'mask': torch.LongTensor(mask),\n",
    "      'labels': torch.Tensor(self.y[index]),\n",
    "      # 'labels': torch.LongTensor([self.y[index]])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mERN4NgElV8Q"
   },
   "outputs": [],
   "source": [
    "# Definition of BERT Classification Model\n",
    "class BERTClass(torch.nn.Module):\n",
    "  def __init__(self, pretrained, pretrained_config, drop_rate, otuput_size):\n",
    "    super().__init__()\n",
    "    self.config = BertConfig.from_pretrained(pretrained_config)\n",
    "    self.bert = BertModel.from_pretrained(pretrained, config = self.config)\n",
    "    self.drop = torch.nn.Dropout(drop_rate)\n",
    "    self.fc = torch.nn.Linear(768, otuput_size)                          # 768 dimensions to match BERT output\n",
    "\n",
    "  def forward(self, ids, mask):\n",
    "    _, out = self.bert(ids, attention_mask=mask, return_dict = False)   # In ver4, it does not work without ”return_dict = False“.\n",
    "    out = self.fc(self.drop(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYjxxMSwlYPP"
   },
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, loader, device, criterion=None):\n",
    "   \"\"\" Calculate loss and accuracy rates\"\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # forward propagation\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      # Calculation of Losses\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # calculation of accuracy \n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()   # Predicted label array for batch size length\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "  return loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def calculate_loss_and_accuracy_test(model, loader, device, criterion=None):\n",
    "  \"\"\" Calculate loss and accuracy rates\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "\n",
    "  logits_list = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      labels = labels.squeeze_() # change\n",
    "\n",
    "      # 順伝播\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      ## for Ensemble soft voting\n",
    "      # outputs2 = outputs.tolist()\n",
    "      logits_list.append(outputs)\n",
    "\n",
    "      # Calculation of Losses\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # calculation of accuracy \n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()  #  Predicted label array for batch size length\n",
    "      y_pred.append(pred)\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      y_true.append(labels)\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  recall = recall_score(y_true, y_pred, average = \"macro\")\n",
    "  precision = precision_score(y_true, y_pred, average = \"macro\")\n",
    "  f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "\n",
    "  return loss / len(loader), correct / total, acc, recall, precision, f1, logits_list\n",
    "\n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
    "  \"\"\"Perform model training and return loss/accuracy logs\"\"\"\n",
    "   # Device Designation\n",
    "  model.to(device)\n",
    "\n",
    "  # Creating a dataloader\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # learning\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # Record start time\n",
    "    s_time = time.time()\n",
    "\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    for data in dataloader_train:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # Initialize slope at zero\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward propagation + error back propagation + weight update\n",
    "      outputs = model(ids, mask)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Calculation of loss and accuracy\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, device, criterion=criterion)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, device, criterion=criterion)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # Save checkpoints\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'{kf}_checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # End time record\n",
    "    e_time = time.time()\n",
    "\n",
    "    # Output log\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6tu23-byv1l5"
   },
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "\n",
    "# tokeinzer settings\n",
    "tagger = MeCab.Tagger(\"-Owakati -d /content/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200910\")\n",
    "tknz = BertJapaneseTokenizer(\"/content/drive/MyDrive/BERT(MeCab+NEologd,語彙数32000,大規模日本語ビジネスニュースコーパス)/PyTorch版/vocab.txt\")\n",
    "\n",
    "# Specify a pre-trained model\n",
    "pretrained_config = \"/content/drive/MyDrive/BERT(MeCab+NEologd,語彙数32000,大規模日本語ビジネスニュースコーパス)/PyTorch版/bert_config.json\"\n",
    "pretrained = \"/content/drive/MyDrive/BERT(MeCab+NEologd,語彙数32000,大規模日本語ビジネスニュースコーパス)/PyTorch版\"\n",
    "\n",
    "# Specify maximum series length\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Category Settings\n",
    "categories = [\"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "# categories = [\"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "# Parameter Setting\n",
    "DROP_RATE = 0.4\n",
    "OUTPUT_SIZE = 10\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 40\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "\n",
    "F1_test_list = []\n",
    "recall_test_list = []\n",
    "precision_test_list = []\n",
    "\n",
    "for kf in range(1, 6):\n",
    "\n",
    "  # # Loading Data Sets\n",
    "  train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train/yamaru_10times20_train_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  train.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid/yamaru_10times20_valid_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  valid.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test/yamaru_10times20_test_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  test.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  # # Loading Data Sets\n",
    "  # train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train_yanagi/10times20_train_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # train.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid_yanagi/10times20_valid_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # valid.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test_yanagi/10times20_test_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # test.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "\n",
    "  # Creating a Dataset\n",
    "  dataset_train = CreateDataset(train['content'], train[categories].values, tknz, MAX_LEN)\n",
    "  dataset_valid = CreateDataset(valid['content'], valid[categories].values, tknz, MAX_LEN)\n",
    "  dataset_test = CreateDataset(test['content'], test[categories].values, tknz, MAX_LEN)\n",
    "\n",
    "\n",
    "  # Model Definition\n",
    "  model = BERTClass(pretrained, pretrained_config, DROP_RATE, OUTPUT_SIZE)\n",
    "\n",
    "  # Definition of loss function\n",
    "  criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "  # Optimizer Definition\n",
    "  optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  # Device Designation\n",
    "  device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "  #  Model Learning\n",
    "  log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)\n",
    "  # torch.save({'epoch': 40, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "  #            f'/content/drive/MyDrive/bert/AA/kfold_torchfile/StockMark_yamaru_yanagi/{kf}_stockmark_yanagi_tknModi2_checkpoint40.pt')\n",
    "\n",
    "# Calculation of accuracy rate\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
    "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "# Calculate the accuracy at the last epoch number\n",
    "  # print(f'Accuracy（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[1]:.3f}')\n",
    "  # print(f'Accuracy（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[1]:.3f}')\n",
    "   print(f'Accuracy  (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[1]:.3f}')\n",
    "\n",
    "  # print(f'Recall （learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[3]:.3f}')\n",
    "  # print(f'Recall （Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[3]:.3f}')\n",
    "  print(f'Recall (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]:.3f}')\n",
    "\n",
    "  # print(f'Precision（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[4]:.3f}') # precisionの計算で0/0のときなる\n",
    "  # print(f'Precision（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[4]:.3f}')\n",
    "  print(f'Precision (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]:.3f}')\n",
    "\n",
    "  # print(f'F1 （learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[5]:.3f}')\n",
    "  # print(f'F1（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[5]:.3f}')\n",
    "  print(f'F1 (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]:.3f}')\n",
    "\n",
    "  recall_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]\n",
    "  precision_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]\n",
    "  f1_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]\n",
    "  recall_test_list.append(recall_test)\n",
    "  precision_test_list.append(precision_test)\n",
    "  F1_test_list.append(f1_test)\n",
    "\n",
    "\n",
    "  ## for Ensemble soft voting\n",
    "  # import torch.nn.functional as F\n",
    "\n",
    "  # logits2 = calculate_loss_and_accuracy_test(model, dataloader_test, device)[6]\n",
    "\n",
    "  # prob_list = []\n",
    "  # for i in range(len(logits2)):\n",
    "  #   prob = F.softmax(logits2[i], dim = -1)\n",
    "  #   prob_list.append(prob.tolist()[0])\n",
    "\n",
    "  # df_prob = pd.DataFrame(prob_list)\n",
    "  # df_prob.to_csv(f\"/content/drive/MyDrive/bert/AA/BERT_Prob/10times20_5fold_test/stockmark_test_prob/StockMark_test{kf}_yanagi_tknModi2_prob.csv\", index = None)\n",
    "\n",
    "\n",
    "print(statistics.mean(recall_test_list))\n",
    "print(statistics.pstdev(recall_test_list))\n",
    "print(statistics.mean(precision_test_list))\n",
    "print(statistics.pstdev(precision_test_list))\n",
    "print(statistics.mean(F1_test_list))\n",
    "print(statistics.pstdev(F1_test_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gmcgLzJota9a"
   },
   "outputs": [],
   "source": [
    "# Log visualization\n",
    "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
    "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
    "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMha2fUOEKCzp1YUh6KoRiy",
   "gpuClass": "premium",
   "machine_shape": "hm",
   "mount_file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
   "provenance": [
    {
     "file_id": "1LOfoTjqBKAtLjJfGigWH77xTN2RcHU8G",
     "timestamp": 1728185948655
    },
    {
     "file_id": "1FXwBVf6cbMY1zR5tibOwtET6uTtP3NHW",
     "timestamp": 1671933583893
    },
    {
     "file_id": "1gf3u9R5YqkA80mWLmiVlvDCOUccTPT8M",
     "timestamp": 1658111712498
    },
    {
     "file_id": "1UfPHYepHyTwZTG9S8NgW0TWqWlxA-eDr",
     "timestamp": 1658035404593
    },
    {
     "file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
     "timestamp": 1657279000035
    },
    {
     "file_id": "14T8Vfcch1tu1aa0xfpBMDVrjdF1MrMkw",
     "timestamp": 1656931635933
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
