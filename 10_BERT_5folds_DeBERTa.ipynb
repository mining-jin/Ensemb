{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29777,
     "status": "ok",
     "timestamp": 1727493547482,
     "user": {
      "displayName": "kanda taisei",
      "userId": "12664805990869041403"
     },
     "user_tz": -540
    },
    "id": "k2bvGATwoOAf",
    "outputId": "6c3bb2a2-6c7f-4d3f-ab1f-83b8ffbb6a19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# This code was written by Taisei KANDA.\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swxMLdRV2v0l"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers[\"ja\"]\n",
    "!pip install transformers\n",
    "!pip install mecab-python3\n",
    "!pip install ipadic\n",
    "!pip install unidic-lite\n",
    "!pip install fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9jZWL_f9bg5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertJapaneseTokenizer, BertModel, DebertaV2Model\n",
    "import MeCab\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW1YOjUClS0A"
   },
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class CreateDataset(Dataset):\n",
    "  def __init__(self, X, y, tokenizer, max_len):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):  # len(Dataset) to specify the value to return\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):  # Specify the value to return in Dataset[index]\n",
    "    text = self.X[index]\n",
    "\n",
    "    sen = tagger.parse(text)\n",
    "\n",
    "    inputs = self.tokenizer.encode_plus(\n",
    "      sen,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      # pad_to_max_length=True\n",
    "      truncation = True,  # for transfomers latest ver\n",
    "      padding = \"max_length\" # for transfomers latest ver\n",
    "    )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "\n",
    "    return {\n",
    "      'ids': torch.LongTensor(ids),\n",
    "      'mask': torch.LongTensor(mask),\n",
    "      'labels': torch.Tensor(self.y[index])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mERN4NgElV8Q"
   },
   "outputs": [],
   "source": [
    "# Definition of BERT Classification Model\n",
    "class BERTClass(torch.nn.Module):\n",
    "  def __init__(self, pretrained, drop_rate, otuput_size):\n",
    "    super().__init__()\n",
    "    # self.bert = BertModel.from_pretrained(pretrained)\n",
    "    self.bert = DebertaV2Model.from_pretrained(pretrained)\n",
    "    self.drop = torch.nn.Dropout(drop_rate)\n",
    "    self.fc = torch.nn.Linear(768, otuput_size)  # 768 dimensions to match BERT output\n",
    "\n",
    "  def forward(self, ids, mask):\n",
    "    # _, out = self.bert(ids, attention_mask=mask, return_dict = False) # In ver4, it does not work without “”return_dict = False“”.\n",
    "    out = self.bert(ids, attention_mask=mask, return_dict = False) # In ver4, it does not work without “”return_dict = False“”.\n",
    "    cls_last_hidden_layer = out[0][:,0,:]\n",
    "    out = self.fc(self.drop(out[0][:,0,:]))\n",
    "    return out, cls_last_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYjxxMSwlYPP"
   },
   "outputs": [],
   "source": [
    "# Calculate loss and correct answer rate\n",
    "def calculate_loss_and_accuracy(model, loader, device, criterion=None):\n",
    "Percentage accuracy calculation\n",
    "def calculate_loss_and_accuracy(model, loader, device, criterion=None):\n",
    "  \"\"\" Calculate loss and accuracy rates\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # forward propagation\n",
    "      outputs, cls_vectors = model(ids, mask)\n",
    "\n",
    "      # Loss Calculation\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # Percentage accuracy calculation\n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy() # Predicted label array of batch size length\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "  return loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def calculate_loss_and_accuracy_test(model, loader, device, criterion=None):\n",
    "  \"\"\" Calculate loss and accuracy answer rate\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "\n",
    "  logits_list = []\n",
    "  cls_last_layer = []\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # forward propagation\n",
    "      outputs, cls_vectors = model(ids, mask)\n",
    "\n",
    "      # for Ensemble soft voting\n",
    "      # outputs2 = outputs.tolist()\n",
    "      logits_list.append(outputs)\n",
    "\n",
    "      cls_last_layer.append(cls_vectors.tolist())\n",
    "\n",
    "      # Loss Calculation\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # Percentage Correct Calculation\n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy() # Predicted label array of batch size length\n",
    "      y_pred.append(pred)\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      y_true.append(labels)\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  recall = recall_score(y_true, y_pred, average = \"macro\")\n",
    "  precision = precision_score(y_true, y_pred, average = \"macro\")\n",
    "  f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "\n",
    "  return loss / len(loader), correct / total, acc, recall, precision, f1, logits_list, y_true, cls_last_layer # Change 1/20\n",
    "\n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
    "  \"\"\"Performs model training and returns log of losses and accuracy\"\"\"\n",
    "  # Device Designation\n",
    "  model.to(device)\n",
    "\n",
    "  # Creating a dataloader\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # learning\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # Record start time\n",
    "    s_time = time.time()\n",
    "\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    for data in dataloader_train:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "\n",
    "      # Initialize slope at zero\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward propagation + error back propagation + weight update\n",
    "      outputs, cls_vectors = model(ids, mask)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Calculation of Losses and Correct Percentages\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, device, criterion=criterion)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, device, criterion=criterion)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # Save checkpoints\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'{kf}_checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # End time record\n",
    "    e_time = time.time()\n",
    "\n",
    "    # Output log\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tu23-byv1l5"
   },
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "\n",
    "\n",
    "# Specify a pre-trained model\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Owakati -d /usr/local/lib/python3.10/dist-packages/unidic_lite/dicdir\")\n",
    "pretrained = 'ku-nlp/deberta-v2-base-japanese'\n",
    "\n",
    "# Specify maximum series length\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Acquisition of tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-base-japanese')\n",
    "\n",
    "# Category Settings\n",
    "categories = [\"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "# categories = [\"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "# Parameter Setting\n",
    "DROP_RATE = 0.4\n",
    "OUTPUT_SIZE = 10\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 40\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "\n",
    "F1_test_list = []\n",
    "recall_test_list = []\n",
    "precision_test_list = []\n",
    "\n",
    "for kf in range(1, 6):\n",
    "\n",
    "  # # Loading Data Sets\n",
    "  train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train/yamaru_10times20_train_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  train.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid/yamaru_10times20_valid_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  valid.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test/yamaru_10times20_test_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  test.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  # # Loading Data Sets\n",
    "  # train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train_yanagi/10times20_train_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # train.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid_yanagi/10times20_valid_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # valid.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test_yanagi/10times20_test_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # test.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "\n",
    "  # Creating a Dataset\n",
    "  dataset_train = CreateDataset(train['content'], train[categories].values, tokenizer, MAX_LEN)\n",
    "  dataset_valid = CreateDataset(valid['content'], valid[categories].values, tokenizer, MAX_LEN)\n",
    "  dataset_test = CreateDataset(test['content'], test[categories].values, tokenizer, MAX_LEN)\n",
    "\n",
    "Creating a Dataset\n",
    "  # Model Definition\n",
    "  model = BERTClass(pretrained, DROP_RATE, OUTPUT_SIZE)\n",
    "\n",
    "  # Definition of loss function\n",
    "  criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "  # Optimizer Definition\n",
    "  optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  # Device Designation\n",
    "  device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "  # Model Learning\n",
    "  log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)\n",
    "  # torch.save({'epoch': 40, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "  #            f'/content/drive/MyDrive/bert/AA/kfold_torchfile/DeBERTa_yamaru_yanagi/{kf}_DeBERTa_checkpoint40_yanagi.pt') # 最後のエポックの重みを保存\n",
    "\n",
    "\n",
    "  # Calculation of percentage of correct answers\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
    "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "  # Calculate the percentage of correct answers in the last epoch\n",
    "  # print(f'Accuracy（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[1]:.3f}')\n",
    "  #print(f'Accuracy（Verification data）：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[1]:.3f}')\n",
    "   print(f'Accuracy (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[1]:.3f}')\n",
    "\n",
    "  # print(f'Recall (learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[3]:.3f}')\n",
    "  # print(f'Recall（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[3]:.3f}')\n",
    "   print(f'Recall (Evaluation Data) {calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]:.3f}')\n",
    "\n",
    "  # print(f'Precision（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[4]:.3f}') # precisionの計算で0/0のときなる\n",
    "  # print(f'Precision（Verification dataVerification data）：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[4]:.3f}')\n",
    "  print(f'Precision (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]:.3f}')\n",
    "\n",
    "  # print(f'F1（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[5]:.3f}')\n",
    "  # print(f'F1（Verification data）：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[5]:.3f}')\n",
    "  print(f'F1（Evaluation Data）：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]:.3f}')\n",
    "\n",
    "\n",
    "  recall_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]\n",
    "  precision_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]\n",
    "  f1_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]\n",
    "\n",
    "  # print(f\"logits F1{f1_test}\")\n",
    "  recall_test_list.append(recall_test)\n",
    "  precision_test_list.append(precision_test)\n",
    "  F1_test_list.append(f1_test)\n",
    "\n",
    "\n",
    "  # # # ## for Ensemble soft voting 2022/07/19\n",
    "  # import torch.nn.functional as F\n",
    "\n",
    "  # logits2 = calculate_loss_and_accuracy_test(model, dataloader_test, device)[6] # Matrix of logits for each individual\n",
    "\n",
    "  # prob_list = []\n",
    "  # for i in range(len(logits2)):\n",
    "  #   prob = F.softmax(logits2[i], dim = -1) # Converted to probability value\n",
    "  #   prob_list.append(prob.tolist()[0])\n",
    "\n",
    "  # df_prob = pd.DataFrame(prob_list)\n",
    "  # df_prob.to_csv(f\"/content/drive/MyDrive/bert/AA/BERT_Prob/10times20_5fold_test/DeBERTa_test_prob/DeBERTa_test{kf}_prob_MeCabver.csv\", index = None)\n",
    "\n",
    "print(statistics.mean(recall_test_list))\n",
    "print(statistics.pstdev(recall_test_list))\n",
    "print(statistics.mean(precision_test_list))\n",
    "print(statistics.pstdev(precision_test_list))\n",
    "print(statistics.mean(F1_test_list))\n",
    "print(statistics.pstdev(F1_test_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmcgLzJota9a"
   },
   "outputs": [],
   "source": [
    "# Log Visualization\n",
    "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
    "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
    "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOYaE9+nRlN0mxyd54BtGzv",
   "gpuClass": "premium",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
   "provenance": [
    {
     "file_id": "1PJNGcZ7zWr4fb9899geqx1EQa12zh4J0",
     "timestamp": 1728185231318
    },
    {
     "file_id": "1N95rXjI8tpGrA3OzF6g0aY7ERsfYZRZG",
     "timestamp": 1723356491668
    },
    {
     "file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
     "timestamp": 1723287413017
    },
    {
     "file_id": "14T8Vfcch1tu1aa0xfpBMDVrjdF1MrMkw",
     "timestamp": 1656931635933
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
