{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25527,
     "status": "ok",
     "timestamp": 1723288289651,
     "user": {
      "displayName": "kanda taisei",
      "userId": "12664805990869041403"
     },
     "user_tz": -540
    },
    "id": "k2bvGATwoOAf",
    "outputId": "7b6dd955-5381-4623-9804-accc71b7021b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# This code was written by Taisei KANDA.\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grAPmjbwJeqn"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swxMLdRV2v0l"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[\"ja\"]\n",
    "!pip install mecab-python3\n",
    "!pip install ipadic\n",
    "!pip install unidic-lite\n",
    "!pip install fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9jZWL_f9bg5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW1YOjUClS0A"
   },
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class CreateDataset(Dataset):\n",
    "  def __init__(self, X, y, tokenizer, max_len):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):              # len(Dataset) to specify the value to return\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):  # Specify the value to return in Dataset[index]\n",
    "    text = self.X[index]\n",
    "    inputs = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      # pad_to_max_length=True\n",
    "      truncation = True,  # for transfomers latest ver\n",
    "      padding = \"max_length\" # for transfomers latest ver\n",
    "    )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "\n",
    "    return {\n",
    "      'ids': torch.LongTensor(ids),\n",
    "      'mask': torch.LongTensor(mask),\n",
    "      'labels': torch.Tensor(self.y[index])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mERN4NgElV8Q"
   },
   "outputs": [],
   "source": [
    "# Definition of BERT Classification Model\n",
    "class BERTClass(torch.nn.Module):\n",
    "  def __init__(self, pretrained, drop_rate, otuput_size):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained(pretrained)\n",
    "    self.drop = torch.nn.Dropout(drop_rate)\n",
    "    self.fc = torch.nn.Linear(768, otuput_size)                       #  768 dimensions to match BERT output\n",
    "\n",
    "  def forward(self, ids, mask):\n",
    "    _, out = self.bert(ids, attention_mask=mask, return_dict = False) #  In ver4, it does not work without ”return_dict = False“.\n",
    "    out = self.fc(self.drop(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYjxxMSwlYPP"
   },
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, loader, device, criterion=None):\n",
    "  \"\"\" Calculate loss and accuracy rates\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # forward propagation\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      # Loss Calculation\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # Percentage accuracy calculation\n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()   # Predicted label array for batch size length\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "  return loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def calculate_loss_and_accuracy_test(model, loader, device, criterion=None):\n",
    "  \"\"\" Calculate loss and accuracy rates\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "\n",
    "  logits_list = []\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # forward propagation\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      ## for Ensemble soft voting\n",
    "      # outputs2 = outputs.tolist()\n",
    "      logits_list.append(outputs)\n",
    "\n",
    "      #  Loss Calculation\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # Precision Calculation\n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()   # Predicted label array of batch size length\n",
    "      y_pred.append(pred)\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      y_true.append(labels)\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  recall = recall_score(y_true, y_pred, average = \"macro\")\n",
    "  precision = precision_score(y_true, y_pred, average = \"macro\")\n",
    "  f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "\n",
    "  return loss / len(loader), correct / total, acc, recall, precision, f1, logits_list, y_true    # Change 1/20\n",
    "\n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
    "  \"\"\"Perform model training and return loss and accuracy rate logs.\"\"\"\n",
    "  # Device Designation\n",
    "  model.to(device)\n",
    "\n",
    "  # Creating a dataloader\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # learning\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # Record start time\n",
    "    s_time = time.time()\n",
    "\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    for data in dataloader_train:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "\n",
    "      # Initialize slope at zero\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward propagation + error back propagation + weight update\n",
    "      outputs = model(ids, mask)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Calculation of Losses and Correct Percentages\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, device, criterion=criterion)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, device, criterion=criterion)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # Save checkpoints\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'{kf}_checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # End time record\n",
    "    e_time = time.time()\n",
    "\n",
    "    # Output log\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tu23-byv1l5"
   },
   "outputs": [],
   "source": [
    "# Cross-validation method\n",
    "\n",
    "\n",
    "# Specify pre-trained model\n",
    "pretrained = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "# Specify maximum series length\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Get tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained)\n",
    "\n",
    "# Set categories\n",
    "categories = [\"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "# categories = [\"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "# Set parameters\n",
    "DROP_RATE = 0.4\n",
    "OUTPUT_SIZE = 10\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 40\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "\n",
    "F1_test_list = []\n",
    "recall_test_list = []\n",
    "precision_test_list = []\n",
    "\n",
    "for kf in range(1, 6):\n",
    "\n",
    "  # # # Load dataset\n",
    "  train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train/yamaru_10times20_train_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  train.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid/yamaru_10times20_valid_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  valid.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test/yamaru_10times20_test_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  test.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  # # # Load dataset\n",
    "  # train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train_yanagi/10times20_train_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # train.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid_yanagi/10times20_valid_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # valid.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test_yanagi/10times20_test_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # test.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "\n",
    "  # Datasetの作成\n",
    "  dataset_train = CreateDataset(train['content'], train[categories].values, tokenizer, MAX_LEN)\n",
    "  dataset_valid = CreateDataset(valid['content'], valid[categories].values, tokenizer, MAX_LEN)\n",
    "  dataset_test = CreateDataset(test['content'], test[categories].values, tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "  # Model Definition\n",
    "  model = BERTClass(pretrained, DROP_RATE, OUTPUT_SIZE)\n",
    "\n",
    "  # Definition of loss function\n",
    "  criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "  # Optimizer Definition\n",
    "  optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  # Device Designation\n",
    "  device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "  # Model Learning\n",
    "  log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)\n",
    "  # torch.save({'epoch': 40, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "  #            f'/content/drive/MyDrive/bert/AA/kfold_torchfile/Tohoku_yamaru/{kf}_tohoku_checkpoint40.pt')    # Save the weight of the last epoch\n",
    "\n",
    "\n",
    "  # Calculation of accuracy rate\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
    "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "  # 最後のエポック数における正解率を計算\n",
    "  # print(f'accuracy（learning data）（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[1]:.3f}')\n",
    "  # print(f'accuracy（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[1]:.3f}')\n",
    "   print(f'accuracy  (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[1]:.3f}')\n",
    "\n",
    "  # print(f'Recall （learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[3]:.3f}')\n",
    "  # print(f'Recall （Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[3]:.3f}')\n",
    "  print(f'Recall (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]:.3f}')\n",
    "\n",
    "  # print(f'Precision（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[4]:.3f}') \n",
    "  # print(f'Precision（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[4]:.3f}')\n",
    "  print(f'Precision (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]:.3f}')\n",
    "\n",
    "  # print(f'F1 （learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[5]:.3f}')\n",
    "  # print(f'F1（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[5]:.3f}')\n",
    "  print(f'F1 (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]:.3f}')\n",
    "\n",
    "\n",
    "  recall_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]\n",
    "  precision_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]\n",
    "  f1_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]\n",
    "\n",
    "  # print(f\"logits F1{f1_test}\")\n",
    "  recall_test_list.append(recall_test)\n",
    "  precision_test_list.append(precision_test)\n",
    "  F1_test_list.append(f1_test)\n",
    "\n",
    "\n",
    "  # ## for Ensemble soft voting\n",
    "  # import torch.nn.functional as F\n",
    "\n",
    "  # logits2 = calculate_loss_and_accuracy_test(model, dataloader_test, device)[6] # 各個体のlogitsの行列\n",
    "\n",
    "  # prob_list = []\n",
    "  # for i in range(len(logits2)):\n",
    "  #   prob = F.softmax(logits2[i], dim = -1) # Converted to probability value\n",
    "  #   prob_list.append(prob.tolist()[0])\n",
    "\n",
    "  # # df_prob = pd.DataFrame(prob_list)\n",
    "  # # df_prob.to_csv(f\"/content/drive/MyDrive/bert/AA/BERT_Prob/10times20_5fold_test/tohoku_test_prob/Tohoku_test{kf}_prob.csv\", index = None)\n",
    "\n",
    "\n",
    "\n",
    "  ## for Ensemble soft voting 2023/01/20\n",
    "  # import torch.nn.functional as F\n",
    "\n",
    "  # logits2 = calculate_loss_and_accuracy_test(model, dataloader_test, device)[6]    # Matrix of logits for each individual\n",
    "  # y_true2 = calculate_loss_and_accuracy_test(model, dataloader_test, device)[7]\n",
    "\n",
    "  # pred_list3 = []\n",
    "  # for i in range(len(logits2)):\n",
    "  #   prob3 = F.softmax(logits2[i], dim = -1)              # Converted to probability value\n",
    "  #   pred3 = torch.argmax(prob3, dim=-1).cpu().numpy()    # Change 1/20\n",
    "  #   pred_list3.append(pred3)\n",
    "\n",
    "  # f1_2 = f1_score(y_true2, pred_list3, average = \"macro\")\n",
    "  # print(f\"softmax F1 {f1_2}\")\n",
    "\n",
    "print(statistics.mean(recall_test_list))\n",
    "print(statistics.pstdev(recall_test_list))\n",
    "print(statistics.mean(precision_test_list))\n",
    "print(statistics.pstdev(precision_test_list))\n",
    "print(statistics.mean(F1_test_list))\n",
    "print(statistics.pstdev(F1_test_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmcgLzJota9a"
   },
   "outputs": [],
   "source": [
    "# Log visualization\n",
    "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
    "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
    "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP0uVvPkNO6AOa7n8Ix0pRg",
   "gpuClass": "premium",
   "machine_shape": "hm",
   "mount_file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
   "provenance": [
    {
     "file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
     "timestamp": 1728185744998
    },
    {
     "file_id": "14T8Vfcch1tu1aa0xfpBMDVrjdF1MrMkw",
     "timestamp": 1656931635933
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
