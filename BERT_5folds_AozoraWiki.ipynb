{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17108,
     "status": "ok",
     "timestamp": 1727185888004,
     "user": {
      "displayName": "kanda taisei",
      "userId": "12664805990869041403"
     },
     "user_tz": -540
    },
    "id": "k2bvGATwoOAf",
    "outputId": "947ef73f-9827-4a3c-c40b-637f0929e57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# This code was written by Taisei KANDA.\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1m7Y1C7PKJb"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers[\"ja\"]\n",
    "!pip install transformers\n",
    "# Mecab installation, ipadic, mecab-python3 not working without unidic-lite\n",
    "!pip install mecab-python3\n",
    "!pip install ipadic\n",
    "!pip install unidic-lite\n",
    "!pip install fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9jZWL_f9bg5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertJapaneseTokenizer\n",
    "import MeCab\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW1YOjUClS0A"
   },
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class CreateDataset(Dataset):\n",
    "  def __init__(self, X, y, tokenizer, max_len):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):              # len(Dataset) to specify the value to return\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):  # Specify the value to return in Dataset[index]\n",
    "    text = self.X[index]\n",
    "\n",
    "    # 2022/12/13 BertJapaneseTokenzier can remove [UNK] after changing to BertJapaneseTokenzier.\n",
    "    sen = tagger.parse(text)\n",
    "\n",
    "    inputs = self.tokenizer.encode_plus(\n",
    "      sen,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      # pad_to_max_length=True\n",
    "      truncation = True,           # for transfomers latest ver\n",
    "      padding = \"max_length\"       # for transfomers latest ver\n",
    "    )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "\n",
    "    return {\n",
    "      'ids': torch.LongTensor(ids),\n",
    "      'mask': torch.LongTensor(mask),\n",
    "      'labels': torch.Tensor(self.y[index])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mERN4NgElV8Q"
   },
   "outputs": [],
   "source": [
    "# Definition of BERT classification model\n",
    "class BERTClass(torch.nn.Module):\n",
    "  def __init__(self, pretrained, pretrained_config, drop_rate, otuput_size):\n",
    "    super().__init__()\n",
    "    self.config = BertConfig.from_pretrained(pretrained_config)\n",
    "    self.bert = BertModel.from_pretrained(pretrained, config = self.config)\n",
    "    self.drop = torch.nn.Dropout(drop_rate)\n",
    "    self.fc = torch.nn.Linear(768, otuput_size)                       #768 dimensions to match BERT output\n",
    "\n",
    "  def forward(self, ids, mask):\n",
    "    _, out = self.bert(ids, attention_mask=mask, return_dict = False) #In ver4, it does not work without “”return_dict = False“”.\n",
    "    out = self.fc(self.drop(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYjxxMSwlYPP"
   },
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, loader, device, criterion=None):\n",
    "  \"\"\" Calculate loss and accuracy rates\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # forward propagation\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      # Calculation of Losses\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # calculation of accuracy \n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()   # Predicted label array for batch size length\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "  return loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def calculate_loss_and_accuracy_test(model, loader, device, criterion=None):\n",
    "  \"\"\" Calculate loss and accuracy rates\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "\n",
    "  logits_list = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # forward propagation\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      ## for Ensemble soft voting\n",
    "      # outputs2 = outputs.tolist()\n",
    "      logits_list.append(outputs)\n",
    "\n",
    "      # Calculation of Losses\n",
    "      if criterion != None:\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # calculation of accuracy \n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()    # Predicted label array for batch size length\n",
    "      y_pred.append(pred)\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # Batch size length correct label array\n",
    "      y_true.append(labels)\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  recall = recall_score(y_true, y_pred, average = \"macro\")\n",
    "  precision = precision_score(y_true, y_pred, average = \"macro\")\n",
    "  f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "\n",
    "  return loss / len(loader), correct / total, acc, recall, precision, f1, logits_list\n",
    "\n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
    "  \"\"\"Perform model training and return loss/accuracy logs\"\"\"\n",
    "   # Device Designation\n",
    "  model.to(device)\n",
    "\n",
    "  # Creating a dataloader\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # learning\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # Record start time\n",
    "    s_time = time.time()\n",
    "\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    for data in dataloader_train:\n",
    "      # Device Designation\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # Initialize slope at zero\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward propagation + error back propagation + weight update\n",
    "      outputs = model(ids, mask)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Calculation of loss and accuracy\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataloader_train, device, criterion=criterion)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataloader_valid, device, criterion=criterion)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # Save checkpoints\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'{kf}_checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # End time record\n",
    "    e_time = time.time()\n",
    "\n",
    "    # Output log\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tu23-byv1l5"
   },
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "\n",
    "# tokeinzer settings\n",
    "tagger = MeCab.Tagger(\"-Owakati -d /usr/local/lib/python3.10/dist-packages/unidic_lite/dicdir\")\n",
    "# tknz = BertTokenizer(\"/content/drive/MyDrive/BERT-base_aozora-jawiki3m_unidic_bpe-32k_2m/vocab.txt\")\n",
    "tknz = BertJapaneseTokenizer(\"/content/drive/MyDrive/BERT-base_aozora-jawiki3m_unidic_bpe-32k_2m/vocab.txt\")\n",
    "\n",
    "# Specify a pre-trained model\n",
    "pretrained_config = \"/content/drive/MyDrive/BERT-base_aozora-jawiki3m_unidic_bpe-32k_2m/bert_config.json\"\n",
    "pretrained = \"/content/drive/MyDrive/BERT-base_aozora-jawiki3m_unidic_bpe-32k_2m\"\n",
    "\n",
    "# Specify maximum series length\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Category Settings\n",
    "categories = [\"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "# categories = [\"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "# Parameter Setting\n",
    "DROP_RATE = 0.4\n",
    "OUTPUT_SIZE = 10\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 40\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "\n",
    "F1_test_list = []\n",
    "recall_test_list = []\n",
    "precision_test_list = []\n",
    "\n",
    "for kf in range(1, 6):\n",
    "\n",
    "  # # Loading Data Sets\n",
    "  train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train/yamaru_10times20_train_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  train.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid/yamaru_10times20_valid_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  valid.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test/yamaru_10times20_test_c_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  test.columns = [\"author\", \"label\", \"content\", \"akutagawa\", \"izumi\", \"kikuchi\", \"mori\", \"natsume\", \"sasaki\", \"shimazaki\",\"dazai\", \"okamoto\",\"umino\"]\n",
    "\n",
    "  # # Loading Data Sets\n",
    "  # train = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_train_yanagi/10times20_train_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # train.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # valid = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_valid_yanagi/10times20_valid_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # valid.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "  # test = pd.read_csv(f\"/content/drive/MyDrive/bert/AA/kfold_tsvfile/5fold_yamaru_test_yanagi/10times20_test_yanagi_noIwai_kfold{kf}.tsv\", sep = \"\\t\", encoding = \"CP932\", header = None)\n",
    "  # test.columns = [\"author\", \"label\", \"content\", \"suzuki\", \"kishi\", \"yoshida\", \"miyabe\", \"morimi\", \"ishida\", \"murakamiharuki\", \"murakami\", \"higashino\", \"minato\"]\n",
    "\n",
    "\n",
    "  # Creating a Dataset\n",
    "  dataset_train = CreateDataset(train['content'], train[categories].values, tknz, MAX_LEN)\n",
    "  dataset_valid = CreateDataset(valid['content'], valid[categories].values, tknz, MAX_LEN)\n",
    "  dataset_test = CreateDataset(test['content'], test[categories].values, tknz, MAX_LEN)\n",
    "\n",
    "\n",
    "  # Model Definition\n",
    "  model = BERTClass(pretrained, pretrained_config, DROP_RATE, OUTPUT_SIZE)\n",
    "\n",
    "  # Definition of loss function\n",
    "  criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "  # Optimizer Definition\n",
    "  optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  # Device Designation\n",
    "  device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "  #  Model Learning\n",
    "  log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)\n",
    "  # torch.save({'epoch': 40, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "  #   f'/content/drive/MyDrive/bert/AA/kfold_torchfile/AozoraWiki3m_yamaru_yanagi/{kf}_aozorawiki3m_yanagi_tknModi2_checkpoint40.pt') \n",
    "  # Save the weight of the last epoch\n",
    "\n",
    "\n",
    "# Calculation of accuracy rate\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
    "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "  # Calculate the accuracy at the last epoch number\n",
    "  # print(f'Accuracy（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[1]:.3f}')\n",
    "  # print(f'Accuracy（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[1]:.3f}')\n",
    "   print(f'Accuracy  (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[1]:.3f}')\n",
    "\n",
    "  # print(f'Recall （learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[3]:.3f}')\n",
    "  # print(f'Recall （Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[3]:.3f}')\n",
    "  print(f'Recall (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]:.3f}')\n",
    "\n",
    "  # print(f'Precision（learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[4]:.3f}')  \n",
    "  # print(f'Precision（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[4]:.3f}')\n",
    "  print(f'Precision (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]:.3f}')\n",
    "\n",
    "  # print(f'F1 （learning data）：{calculate_loss_and_accuracy_test(model, dataloader_train, device)[5]:.3f}')\n",
    "  # print(f'F1（Verification data)：{calculate_loss_and_accuracy_test(model, dataloader_valid, device)[5]:.3f}')\n",
    "  print(f'F1 (Evaluation Data)：{calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]:.3f}')\n",
    "\n",
    "  recall_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[3]\n",
    "  precision_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[4]\n",
    "  f1_test = calculate_loss_and_accuracy_test(model, dataloader_test, device)[5]\n",
    "  recall_test_list.append(recall_test)\n",
    "  precision_test_list.append(precision_test)\n",
    "  F1_test_list.append(f1_test)\n",
    "\n",
    "\n",
    "  # # for Ensemble soft voting\n",
    "  # import torch.nn.functional as F\n",
    "\n",
    "  # logits2 = calculate_loss_and_accuracy_test(model, dataloader_test, device)[6]\n",
    "\n",
    "  # prob_list = []\n",
    "  # for i in range(len(logits2)):\n",
    "  #   prob = F.softmax(logits2[i], dim = -1)\n",
    "  #   prob_list.append(prob.tolist()[0])\n",
    "\n",
    "  # df_prob = pd.DataFrame(prob_list)\n",
    "  # df_prob.to_csv(f\"/content/drive/MyDrive/bert/AA/BERT_Prob/10times20_5fold_test/aozorawiki3m_test_prob/AozoraWiki3m_test{kf}_yanagi_tknModi2_prob.csv\", index = None)\n",
    "\n",
    "\n",
    "print(statistics.mean(recall_test_list))\n",
    "print(statistics.pstdev(recall_test_list))\n",
    "print(statistics.mean(precision_test_list))\n",
    "print(statistics.pstdev(precision_test_list))\n",
    "print(statistics.mean(F1_test_list))\n",
    "print(statistics.pstdev(F1_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmcgLzJota9a"
   },
   "outputs": [],
   "source": [
    "# Log visualization\n",
    "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
    "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
    "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkgh9bVIDIl8fBJZNYqYDF",
   "gpuClass": "premium",
   "machine_shape": "hm",
   "mount_file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
   "provenance": [
    {
     "file_id": "1lSWc4A3ebe-BlAiLpZ0vsof5nNlxMipw",
     "timestamp": 1728185575177
    },
    {
     "file_id": "1gf3u9R5YqkA80mWLmiVlvDCOUccTPT8M",
     "timestamp": 1666068988967
    },
    {
     "file_id": "1UfPHYepHyTwZTG9S8NgW0TWqWlxA-eDr",
     "timestamp": 1658035404593
    },
    {
     "file_id": "1BgHaP5RW7EcETiAg3x7t4rzzA3_9ef4r",
     "timestamp": 1657279000035
    },
    {
     "file_id": "14T8Vfcch1tu1aa0xfpBMDVrjdF1MrMkw",
     "timestamp": 1656931635933
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
